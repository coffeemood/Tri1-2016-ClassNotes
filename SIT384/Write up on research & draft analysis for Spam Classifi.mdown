# Write up on research & draft analysis for Spam Classification problem 

#### A dry run Naive Bayes on the two data set 

In this chapter I built my classifier based on naive bayes word occurrence for both phishing and non-phishing data set. The processing procedure involved  stripping white space, removing stop words, numbers and punctuation, etc... The classifier was then built based on word occurrence, with the addition of the `c` value used for terms that haven't been seen before. 

The usual data processing code 

```r
formatting = function(docs) {
+ docs = tm_map(docs, removePunctuation)
+ docs = tm_map(docs, removeNumbers)
+ docs = tm_map(docs, removeWords, stopwords("english"))
+ docs = tm_map(docs, stripWhitespace)
+ docs = tm_map(docs, tolower)
+ docs = tm_map(docs, PlainTextDocument)
+ }
```


#### SVM using `caret`

> Preprocessing Continued... 

```r

for (i in top20p)
{
    name = paste('freq_',i,sep="")
    all.df[,name] = sapply(1:nrow(all.df),
                    function(z)
                    {
                     100 * length(grep(i,all.df$text[z], ignor.case = TRUE)) / (sapply(gregexpr("\\W+", all.df$text[z]), length) + 1)
                        })
}

for (i in top20np)
{
    name = paste('freq_',i,sep="")
    all.df[,name] = sapply(1:nrow(all.df),
                    function(z)
                    {
                    100 * length(grep(i,all.df$text[z], ignore.case = TRUE)) / (sapply(gregexpr("\\W+", all.df$text[z]), length) + 1)
                        })
}

# Add http feature 
all.df$http = sapply(1:nrow(all.df),
                    function(z)
                    {
                    length(grep("http",all.df$text[z])) 
                        })

# Cleanup for classification 

set.seed(83)
trainIndex = createDataPartition(all.df$type, p = .8, list = FALSE, times = 1)
dataTrain = all.df[trainIndex, ]
dataTest = all.df[-trainIndex, ]
dataTrain$msg = NULL 
dataTrain$text = NULL 
dataTest$msg = NULL 
dataTest$text = NULL 


```

Make a graph to see correlations between certain words and the classified type of message

```r
#step 1: melt all.df with id.var being `type`
#step 2: Plot

ggplot(data= alt.alldf2, aes(x = variable, y = value)) +
  geom_text(aes(color = type,
                               label = type)) +
                               expand_limits(y=0.0005)

```

```r

fitControl <- trainControl(method = "repeatedcv",
                           repeats = 20)

x <- train(type ~ . ,data = dataTrain, 
    na.action = na.omit,
                 method = "svmRadial",
                tuneGrid = svmTuneGrid,
                 trControl = fitControl)

pred <- predict(x,dataTest[,1:28])

acc <- confusionMatrix(pred,dataTest$type)

acc
          Reference
Prediction  n  p
         n 52 12
         p 24 63

               Accuracy : 0.7616
                 95% CI : (0.6855, 0.8271)
    No Information Rate : 0.5033
    P-Value [Acc > NIR] : 6.766e-11

                  Kappa : 0.5237
 Mcnemar's Test P-Value : 0.06675

            Sensitivity : 0.6842
            Specificity : 0.8400
         Pos Pred Value : 0.8125
         Neg Pred Value : 0.7241
             Prevalence : 0.5033
         Detection Rate : 0.3444
   Detection Prevalence : 0.4238
      Balanced Accuracy : 0.7621

       'Positive' Class : n

```

##### Naive Bayes package run 

```r
fitControl <- trainControl(method = "cv",
                           repeats = 20)

x <- train(type ~ . ,data = dataTrain, 
    na.action = na.omit,
                 method = "nb",
                 trControl = fitControl)

pred <- predict(x,dataTest[,1:28])

acc <- confusionMatrix(pred,dataTest$type)

acc

Confusion Matrix and Statistics

          Reference
Prediction  n  p
         n 53 17
         p 23 58

               Accuracy : 0.7351
                 95% CI : (0.6572, 0.8035)
    No Information Rate : 0.5033
    P-Value [Acc > NIR] : 5.199e-09

                  Kappa : 0.4705
 Mcnemar's Test P-Value : 0.4292

            Sensitivity : 0.6974
            Specificity : 0.7733
         Pos Pred Value : 0.7571
         Neg Pred Value : 0.7160
             Prevalence : 0.5033
         Detection Rate : 0.3510
   Detection Prevalence : 0.4636
      Balanced Accuracy : 0.7354

       'Positive' Class : n
```


#### knn Package Run 

```r
fitControl <- trainControl(method = "repeatedcv",
                           repeats = 10)

x <- train(type ~ . ,data = dataTrain, 
    na.action = na.omit,
                 method = "knn",
                 tuneLength = 10,
                 trControl = fitControl)

pred <- predict(x,dataTest[,1:29])

acc <- confusionMatrix(pred,dataTest$type)

acc

Confusion Matrix and Statistics

          Reference
Prediction  n  p
         n 57 22
         p 19 53

               Accuracy : 0.7285
                 95% CI : (0.6502, 0.7976)
    No Information Rate : 0.5033
    P-Value [Acc > NIR] : 1.415e-08

                  Kappa : 0.4568
 Mcnemar's Test P-Value : 0.7548

            Sensitivity : 0.7500
            Specificity : 0.7067
         Pos Pred Value : 0.7215
         Neg Pred Value : 0.7361
             Prevalence : 0.5033
         Detection Rate : 0.3775
   Detection Prevalence : 0.5232
      Balanced Accuracy : 0.7283

       'Positive' Class : n

```

#### randomForest package run 

```r
fitControl <- trainControl(method = "repeatedcv",
                           repeats = 10)

x <- train(type ~ . ,data = dataTrain, 
    na.action = na.omit,
                 method = "rf",
                 tuneLength = 10,
                 trControl = fitControl)

pred <- predict(x,dataTest[,1:29])

acc <- confusionMatrix(pred,dataTest$type)

acc

Confusion Matrix and Statistics

          Reference
Prediction  n  p
         n 60 17
         p 16 58

               Accuracy : 0.7815
                 95% CI : (0.707, 0.8445)
    No Information Rate : 0.5033
    P-Value [Acc > NIR] : 1.798e-12

                  Kappa : 0.5629
 Mcnemar's Test P-Value : 1

            Sensitivity : 0.7895
            Specificity : 0.7733
         Pos Pred Value : 0.7792
         Neg Pred Value : 0.7838
             Prevalence : 0.5033
         Detection Rate : 0.3974
   Detection Prevalence : 0.5099
      Balanced Accuracy : 0.7814

       'Positive' Class : n

```


##### Naive Bayes manual run 

After building the data frames for each of these data set, I ran the phishing data through the two `training.df`, twice with the prior being 0.5 and 0.9 (phishing vs. non-phishing estimation) respectively. Here are the results

```r
classify.msg <- function(path, training.df, prior = 0.5, c = 1e-6)
{
  # Here, we use many of the support functions to get the
  # email text data in a workable format
  msg <- get.msg(path)
  msg.tdm <- get.tdm(msg)
  msg.freq <- rowSums(as.matrix(msg.tdm))
  # Find intersections of words
  msg.match <- intersect(names(msg.freq), training.df$term)
  # Now, we just perform the naive Bayes calculation
  if(length(msg.match) < 1)
  {
    return(prior * c ^ (length(msg.freq)))
  }
  else
  {
    match.probs <- training.df$occurrence[match(msg.match, training.df$term)]
    return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
  }
}

p.pt <- sapply(p.docs,
                           function(p) classify.msg(file.path(p.path, p), training.df = p.df))
p.npt <- sapply(p.docs,
                          function(p) classify.msg(file.path(p.path, p), training.df = np.df))
p.res <- ifelse(p.pt > p.npt,
                      TRUE,
                      FALSE)
summary(p.res)

np.pt <- sapply(np.docs,
                           function(p) classify.msg(file.path(np.path, p), training.df = p.df))

np.npt <- sapply(np.docs,
                          function(p) classify.msg(file.path(np.path, p), training.df = np.df))

np.res <- ifelse(np.pt > np.npt,
                      TRUE,
                      FALSE)
summary(np.res)

```

<table><tr><td colspan="4" align="center" ><b>Result for Phishing</b></td></tr>
<tr><td>Mode</td><td>False</td><td>True</td><td>NA's</td></tr><tr><td>Logical</td><td>345</td><td>34</td><td>0</td></tr></table> 

The result for the phishing data set shows that the prior doesn't have any correlation with the outcome, which isn't that great given that it was fed all phishing messages, in this particular instance 

<table><tr><td colspan="4" align="center" ><b>Result for Non-Phishing</b></td></tr>
<tr><td>Mode</td><td>False</td><td>NA's</td></tr><tr><td>Logical</td><td>380</td><td>0</td></tr></table> 

The classifier performed well on non-phishing data set, giving a 100% accuracy in predicting the 380 non-phishing messages, with the given prior of 0.5.


